{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Workshop\n",
    "### _**Evaluation**_\n",
    "\n",
    "---\n",
    "In this part of the workshop we will get the previous model we trained to Predict Mobile Customer Departure and evaluate its performance with a test dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background) - Getting the model trained in the previous lab.\n",
    "2. [Evaluate](#Evaluate)\n",
    "    * Creating a script to evaluate model\n",
    "    * Using [SageMaker Processing](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) jobs to automate evaluation of models\n",
    "3. [Exercise](#a_Exercise) - customizing metrics and evaluation reports\n",
    "4. [Wrap-up - end of Evaluation Lab](#Wrap-up)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "In the previous [Modeling](../2-Modeling/modeling.ipynb) lab we used SageMaker trained models by creating multiple SageMaker training jobs.\n",
    "\n",
    "Install and import some packages we'll need for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sess = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the variables from initial setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r bucket\n",
    "%store -r prefix\n",
    "%store -r region\n",
    "%store -r docker_image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket, prefix, region, docker_image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### - if you _**skipped**_ the data preparation lab follow instructions:\n",
    "\n",
    "   - **run this [notebook](./config/pre_setup.ipynb)**\n",
    "   - load the model S3 URI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment if you skipped the data preparation lab\n",
    "\n",
    "# %store -r s3uri_model\n",
    "# !cp config/model.tar.gz ./\n",
    "\n",
    "# %store -r s3uri_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### - if you _**have done**_ the previous labs\n",
    "\n",
    "Download the model and test data from S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment if you have done the previous labs\n",
    "\n",
    "# # Get name of training job and other variables\n",
    "# %store -r training_job_name\n",
    "# training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment if you have done the previous labs\n",
    "# estimator = sagemaker.estimator.Estimator.attach(training_job_name)\n",
    "# s3uri_model = estimator.model_data\n",
    "# print(\"\\ns3uri_model =\",s3uri_model)\n",
    "\n",
    "# S3Downloader.download(s3uri_model, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store -r s3uri_test\n",
    "# S3Downloader.download(s3uri_test, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have the `model.tar.gz` file in the 3-Evaluation directory \n",
    "\n",
    "(click the refresh button)\n",
    "\n",
    "![refresh_dir.png](./media/refresh_dir.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model\n",
    "\n",
    "Let's create a simple evaluation with some Scikit-Learn Metrics like [Area Under the Curve (AUC)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html) and [Accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "model_path = \"model.tar.gz\"\n",
    "with tarfile.open(model_path) as tar:\n",
    "    tar.extractall(path=\".\")\n",
    "\n",
    "print(\"Loading xgboost model.\")\n",
    "model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading test input data\")\n",
    "test_path = \"test.csv\"\n",
    "df = pd.read_csv(test_path, header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading test data. We should get an `DMatrix` object...\")\n",
    "y_test = df.iloc[:, 0].to_numpy()\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "X_test = xgboost.DMatrix(df.values)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performing predictions against test data.\")\n",
    "predictions_probs = model.predict(X_test)\n",
    "predictions = predictions_probs.round()\n",
    "\n",
    "print(\"Creating classification evaluation report\")\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "auc = roc_auc_score(y_test, predictions_probs)\n",
    "\n",
    "print(\"Accuracy =\", acc)\n",
    "print(\"AUC =\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a classification report\n",
    "\n",
    "Now, let's save the results in a JSON file, following the structure defined in SageMaker docs:\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html\n",
    "\n",
    "We'll use this logic later in [Lab 6-Pipelines](../6-Pipelines/pipelines.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "# The metrics reported can change based on the model used - check the link for the documentation \n",
    "report_dict = {\n",
    "    \"binary_classification_metrics\": {\n",
    "        \"accuracy\": {\n",
    "            \"value\": acc,\n",
    "            \"standard_deviation\": \"NaN\",\n",
    "        },\n",
    "        \"auc\": {\"value\": auc, \"standard_deviation\": \"NaN\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Classification report:\")\n",
    "pprint.pprint(report_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_output_path = os.path.join(\n",
    "    \".\", \"evaluation.json\"\n",
    ")\n",
    "print(\"Saving classification report to {}\".format(evaluation_output_path))\n",
    "\n",
    "with open(evaluation_output_path, \"w\") as f:\n",
    "    f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ok, now we have working code. Let's put it in a Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluate.py\n",
    "\"\"\"Evaluation script for measuring model accuracy.\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "# May need to import additional metrics depending on what you are measuring.\n",
    "# See https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "\n",
    "def get_dataset(dir_path, dataset_name) -> pd.DataFrame:\n",
    "    files = [ os.path.join(dir_path, file) for file in os.listdir(dir_path) ]\n",
    "    if len(files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(files, dataset_name))\n",
    "    raw_data = [ pd.read_csv(file, header=None) for file in files ]\n",
    "    df = pd.concat(raw_data)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\"..\")\n",
    "\n",
    "    logger.debug(\"Loading xgboost model.\")\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    logger.info(\"Loading test input data\")\n",
    "    test_path = \"/opt/ml/processing/test\"\n",
    "    df = get_dataset(test_path, \"test_set\")\n",
    "\n",
    "    logger.debug(\"Reading test data.\")\n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "\n",
    "    logger.info(\"Performing predictions against test data.\")\n",
    "    predictions_probs = model.predict(X_test)\n",
    "    predictions = predictions_probs.round()\n",
    "\n",
    "    logger.info(\"Creating classification evaluation report\")\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    auc = roc_auc_score(y_test, predictions_probs)\n",
    "\n",
    "    # The metrics reported can change based on the model used, but it must be a specific name per (https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html)\n",
    "    report_dict = {\n",
    "        \"binary_classification_metrics\": {\n",
    "            \"accuracy\": {\n",
    "                \"value\": acc,\n",
    "                \"standard_deviation\": \"NaN\",\n",
    "            },\n",
    "            \"auc\": {\"value\": auc, \"standard_deviation\": \"NaN\"},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    logger.info(\"Classification report:\\n{}\".format(report_dict))\n",
    "\n",
    "    evaluation_output_path = os.path.join(\n",
    "        \"/opt/ml/processing/evaluation\", \"evaluation.json\"\n",
    "    )\n",
    "    logger.info(\"Saving classification report to {}\".format(evaluation_output_path))\n",
    "\n",
    "    with open(evaluation_output_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ok, now we are finally running this script with a simple call to SageMaker Processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    ScriptProcessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing step for evaluation\n",
    "processor = ScriptProcessor(\n",
    "    image_uri=docker_image_name,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"CustomerChurn/eval-script\",\n",
    "    sagemaker_session=sm_sess,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrypoint = \"evaluate.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strftime, gmtime\n",
    "# Helper to create timestamps\n",
    "create_date = lambda: strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.run(\n",
    "    code=entrypoint,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            source=s3uri_model,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            source=s3uri_test,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"\n",
    "        ),\n",
    "    ],\n",
    "    job_name=f\"CustomerChurnEval-{create_date()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well, the SageMaker Processing job must have created the JSON with the evaluation report of our model and saved it in S3.\n",
    "\n",
    "In addition, under the hood, SageMaker Processing has uploaded our `evaluate.py` script to S3. Let's check where the script was saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for proc_in in processor.latest_job.inputs:\n",
    "    if proc_in.input_name == \"code\":\n",
    "        s3_evaluation_code_uri = proc_in.source \n",
    "        \n",
    "s3_evaluation_code_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's store the S3 URI where our evaluation script was saved for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store s3_evaluation_code_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check it the evaluation report from S3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_s3_report_uri = processor.latest_job.outputs[0].destination\n",
    "out_s3_report_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_list = S3Downloader.list(out_s3_report_uri)\n",
    "reports_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = S3Downloader.read_file(reports_list[0])\n",
    "\n",
    "print(\"=====Model Report====\")\n",
    "print(json.dumps(json.loads(report.split('\\n')[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-up\n",
    "\n",
    "Now that we finished the **evaluation lab**, let's make everything here re-usable. It may come in handy later (spoiler alert - when creating Pipelines)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../6-Pipelines/my_labs_solutions/evaluation_solution.py\n",
    "import sagemaker\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    ScriptProcessor,\n",
    ")\n",
    "\n",
    "def get_evaluation_processor(docker_image_name) -> ScriptProcessor:\n",
    "    \n",
    "    role = sagemaker.get_execution_role()\n",
    "    sm_sess = sagemaker.session.Session()\n",
    "\n",
    "    # Processing step for evaluation\n",
    "    processor = ScriptProcessor(\n",
    "        image_uri=docker_image_name,\n",
    "        command=[\"python3\"],\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        instance_count=1,\n",
    "        base_job_name=\"CustomerChurn/eval-script\",\n",
    "        sagemaker_session=sm_sess,\n",
    "        role=role,\n",
    "    )\n",
    "    \n",
    "    return processor"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
